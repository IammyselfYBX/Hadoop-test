如果你有很好的Java基础，当你入门以后，你会感觉到hadoop其实也是很简单的，大数据无非就是数据量大，需要很多机器共同来完成存储工作，云计算无非就是多台机器一起运算。
1新建虚拟机
2配置静态IP
	vi /etc/sysconfig/network-scripts/[网卡名称]
	ONBOOT修改为yes
	其后添加ip地址、掩码、网关（为主机虚拟网卡的IP地址）
	网卡物理地址（ifconfig查询到）
	如下
	BOOTPROTO=static
	ONBOOT=yes
	IPADDR=192.168.10.100    (ip地址)
	NETMASK=255.255.255.0    (子网掩码)
	GATEWAY=192.168.10.1     (网关)
	HWADDR=00:0c:29:ea:cb:d1 (MAC地址)
   	设置好重启network
	#> service network restart
	Restarting network (via systemctl):                        [  OK  ]
3关闭防火墙和SELinux(每台机器都要进行)
	1)关闭防火墙
	先检查防火墙状态
	$> systemctl status firewalld(如图有actice (running)表示防火墙打开)
	关闭
	#> systemctl disable firewalld(执行开机禁用防火墙自启命令)  
    	#> systemctl stop firewalld(如图“防火墙关闭”)
	最后在检查以下
	$> systemctl status firewalld(如图“防火墙关闭状态”有inactice (dead)表示防火墙打开)

	2)关闭SELinux
	查看SELinux状态：
	$> sestatus (如图有enabled表示SELinux打开)
	关闭SELinux
	(1)临时关闭，不用重启机器：
	setenforce 0
	(2)永久关闭，需要重启机器：
	vi /etc/sysconfig/selinux
	将SELINUX=enforcing改为SELINUX=disabled 
	
1安装JDK(以jdk-8u201-linux-x64.tar.gz为例)
	1)卸载自带的jdkopen
	检查已经安装过的jdk：
	$> rpm -qa|grep java
	删除已经安装的文件：
	#> rpm -e --nodeps [安装的jdk包名称]（上个命令查询到的所有jdk文件，即所有以.x86_64结尾的rpm包）	
	1)下载到桌面，解压
	$> tar zxvf jdk-8u201-linux-x64.tar.gz 
	#> mv /home/tom/Desktop/jdk1.8.0_201 /usr/local/
	2)配置Java环境变量
 	#> vi /etc/profile
	在末尾追加(见图片)
	#set Java environment
	JAVA_HOME=/usr/local/jdk1.8.0_201
	PATH=$JAVA_HOME/bin:$PATH
	CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar
	export JAVA_HOME
	export PATH
	export CLASSPATH
	3)配置完成后 source /etc/profile生效
	4)检验
	#> which java
5克隆
6修改主机名（节点增加时也需要进行如上的配置）
	1)vi /etc/sysconfig/network
	HOSTNAME=master (HOSTNAME=slaver1)
	2)vi /etc/hostname(配置域名)
	（要是namenode中：HOSTNAME=master 在datanode中：HOSTNAME=slave1）
	(或者#> hostnamectl set-hostname master)
7修改hosts文件(以便namenode可以通过主机名解析datanode的IP)
	202.204.65.119  master
	202.204.65.40   slave1
	202.204.65.42   slave2
	(节点增加时，往下排列)
ssh 
2配置ssh免密登入
SSH无密码原理：Master（NameNode | JobTracker）作为客户端，要实现无密码公钥认证，连接到服务器Salve（DataNode | Tasktracker）上时，需要在Master上生成一个密钥对，包括一个公钥和一个私钥，而后将公钥复制到所有的Slave上。当Master通过SSH连接Salve时，Salve就会生成一个随机数并用Master的公钥对随机数进行加密，并发送给Master。Master收到加密数之后再用私钥解密，并将解密数回传给Slave，Slave确认解密数无误之后就允许Master进行连接了。这就是一个公钥认证过程，其间不需要用户手工输入密码。重要过程是将客户端Master复制到Slave上。 分为两部分：Master无密码登录所有的Slave 和 所有的
	1)检验是否安装ssh	
	rpm –qa | grep openssh
	rpm –qa | grep rsync
	如果提示没有安装，则需安装，如果安装跳过该步
	#> yum install ssh   #安装SSH协议
	#> yum install rsync  #rsync是一个远程数据同步工具，可通过LAN/WAN快速同步多台主机间的文件）
	#> systemctl restart  sshd.service  #启动服务
	2）这句话一定要看，一定要看，因为关系到配置ssh效率的问题。
	原理说白了就是：需要将Master的公钥id_rsa.pub追加到所有的Slave的authorized_keys里边，将所有的Slave中的id_rsa.pub追加到Master的authorized_keys里边。
	因此先配置Master和所有的Slave机器的sshd_config
	(1)vi /etc/ssh/sshd_config
	修改
	#RSAAuthentication yes      #开启私钥验证
    	#PubkeyAuthentication yes   #开启公钥验证
	成这样     
    	RSAAuthentication yes # 启用 RSA 认证
    	PubkeyAuthentication yes  开启公钥认证

	#启动服务
	然后分别在所有机器用户的~/目录下分别建立.ssh文件夹并将其权限设为700 
	#> chmod 700 ~/.ssh 
	
	#> cd 
	#> mkdir .ssh
  	#> chmod 777 .ssh

	(1)slave1的配置
	#> cd 
	#> cd .ssh
	#> ssh-keygen -t rsa
	#> chmod 700 id*
	#> cat id_rsa.pub >> authorized_keys
	#> cat id_rsa >> authorized_keys
	#> chmod 644 authorized_keys
	#> scp /root/.ssh/authorized_keys slave1:/root/.ssh
	#> scp /root/.ssh/authorized_keys slave2:/root/.ssh
	#> scp /root/.ssh/authorized_keys master:/root/.ssh
	(2)slave2的配置
	#> cd 
	#> cd .ssh
	#> ssh-keygen -t rsa
	#> cd ..	
	#> chmod 700 .ssh
	#> scp /root/.ssh/authorized_keys slave1:/root/.ssh
	#> scp /root/.ssh/authorized_keys master:/root/.ssh
	(3)master的配置
	#> cd 
	#> cd .ssh
	#> ssh-keygen -t rsa
	#> scp /root/.ssh/authorized_keys slave1:/root/.ssh
	#> scp /root/.ssh/authorized_keys slave2:/root/.ssh


3搭建Hadoop
	1)官网下载hadoop(以hadoop-2.7.7为例，适用于hadoop-2.x)
	#> mv hadoop-2.7.7.tar.gz /usr/local
	#> cd /usr/local
	#> tar -zxvf hadoop-2.7.7.tar.gz 


	2)配置环境变量
	#> vi /etc/profile
	(在结尾添加)
	HADOOP_HOME=/usr/local/hadoop-2.7.7
	PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin
	退出vi，使生效
	#> source /etc/profile
	检验环境变量是否匹配好
	#> which hadoop

	3)Hadoop修改java环境
	#> vi /usr/local/hadoop-2.7.7/etc/hadoop/hadoop-env.sh
	(找到export JAVA——HOME=${JAVA_HOME}
	改成export JAVA_HOME=/usr/local/jdk1.8.0_201)
4配置hadoop
	core-site.xml 对应有一个core-default.xml, hdfs-site.xml对应有一个hdfs-default.xml,mapred-site.xml对应有一个mapred-default.xml。这三个defalult文件里面都有一些默认配置，现在我们修改这三个site文件，目的就覆盖default里面的一些配置

	#> cd /usr/local/hadoop-2.7.7/etc/hadoop
	1）core-site.xml，配置内容如下
	#>mkdir -p /home/hadoop/tmp
	<configuration>
    	<!-- 指定HDFS老大（namenode）的通信地址，说白了就是谁是namenode -->
	<property>
   		<name>fs.defaultFS</name>
    		<value>hdfs://hdfs:master:9000</value> <!--master是namenode的主机名；8020是默认端口-->
	</property>
	<!--配置操作Hadoop的缓存大小-->
	<property>
	<name>io.file,buffer.size</name>
	<value>4096</value> <!--Hadoop 2.x默认是4096-->
	</property>
    	<!-- 指定hadoop运行时产生文件的存储路径，临时数据存储目录 -->
	<property>
		<name>hadoop.tmp.dir</name>
		<value>/home/hadoop/tmp</value> <!--需要#>mkdir -p /home/hadoop/tmp,而且不能放到/tmp里面，因为那里面的数据不可控-->
	</property>
	<property>
	<name>fs.default.name</name>
	<value>hdfs:master:9000</value>
	</property>
	<property>
	<name>dfs.namenode.rpc-address</name>
	<value>master:8080</value>
	</property>
	</configuration>

	2)修改hdfs-site.xml，修改配置如下
	<configuration>	
	<!-- 设置hdfs副本数量 -->
	<property>
	    <name>dfs.replication</name>
	    <value>3</value> <!--一般习惯是配置3个副本-->
	</property>
	<!--配置每个块的大小(hadoop文件最小存储单元)-->
	<property>
	<name>dfs.block.size</name>
	<value>134217728</value> <!--Hadoop2.x是128M，Hadoop1.x是64M，这里添加的是字节-->
	</property>
	<!--hdfs的元数据存储位置(元数据:namenode管理的数据，描述datanode的数据)--> 
	<property>
	<name>dfs.namenode.name.dir</name>
	<value>/home/hadoop/hadoopdata/dfs/name</value> <!--需要#>mkdir -p /home/hadoop/hadoopdata/dfs/name-->
	</property>
	<!--hdfs的数据存储位置-->
	<property>
	<name>dfs.datanode.data.dir</name>
	<value>/home/hadoop/hadoopdata/dfs/data</value> <!--需要#>mkdir -p /home/hadoop/hadoopdata/dfs/data-->
	</property>
	<!--hdfs的检测目录-->
	<property>
	<name>fs.checkpoint.dir</name>
	<value>/home/hadoop/hadoopdata/checkpoint/dfs/name</value>
	</property>

	</configuration>

	3)修改mapred-site.xml 由于在配置文件目录下没有，需要修改名称：
	#> cp mapred-site.xml.template mapred-site.xml
	<configuration>
	<!-- 通知框架MapReduce使用YARN -->
	<property>
	    <name>mapreduce.framework.name</name>
	    <value>yarn</value>
	    <final>true</final>
	</property>
	<!--历史服务的通信地址-->
	<property>
	<name>mapreduce.jobhistory.address</name>
	<value>master:10020</value>  <!--这里是resourcemanager-->
	</property>
	<!--历史服务的Web UI的地址-->
	<property>
	<name>mapreduce.jobhistory.webapp.address</name>
	<value>master:19888</value>
	</property>
	<!--设置jobtracker所在机器，端口号9001-->
	<property>
	    <name>mapred.job.tracker</name>
	    <value>master:9001</value>
	  </property>
	</configuration>

	4)修改yarn-site.xml，修改内容如下	
	<configuration>
	<!-- reducer取数据的方式是mapreduce_shuffle -->
	<property>
	    <name>yarn.nodemanager.aux-services</name>
	    <value>mapreduce_shuffle</value>
	</property>
	<property>
	    <name>yarn.resourcemanager.hostname</name>
	    <value>localhost</value>
	</property>
	</configuration>
	
	5将整个hadoop-2.7.7文件夹及其子文件夹使用scp复制到slave1和slave2的相同目录中：
	#> scp -r /usr/local/hadoop-2.7.7 slave1:/usr/local/
	#> scp -r /usr/local/hadoop-2.7.7 slave2:/usr/local/

5启动hadoop

	1)格式化hadoop，进入目录：
	#> /usr/local/hadoop-2.7.7/etc/hadoop
	执行下列之一命令即可
	#> hadoop namenode -format  （过时）
	或
	#> hdfs namenode -format

	2)启动hdfs和yarn
	 (1)先启动HDFS(NameNode,DataNode守护进程)
	 #> cd/usr/local/hadoop-2.7.7
	sbin/start-dfs.sh

	 (2)再启动YARN
	 sbin/start-yarn.sh


	3)验证是否成功，使用命令：jps，输出如下即表示配置成功。	
	#> jps
	12272 Jps
	4135 JobTracker
	9500 SecondaryNameNode
	9943 NodeManager
	9664 ResourceManager
	8898 NameNode
	9174 DataNode

	4)可以在浏览器中查看hdfs和mr的状态.hdfs管理界面：http://localhost:50070  MR的管理界面：http://localhost:8088 

















	

